{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lathe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Chatbot Output:\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load your CSV file (example for one dataset)\n",
    "#    You can load multiple CSVs and merge documents if needed.\n",
    "# -------------------------------\n",
    "# Here we load the \"insurance_plans_by_disease.csv\" file as an example.\n",
    "csv_path = r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_plan_grouping.csv'\n",
    "loader = CSVLoader(file_path=csv_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Split the documents into manageable chunks.\n",
    "# -------------------------------\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create a vector store from the documents using embeddings.\n",
    "#    (Here we use the \"all-MiniLM-L6-v2\" model from sentence-transformers)\n",
    "# -------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Create a retriever from the vector store.\n",
    "# -------------------------------\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Set up a RetrievalQA chain using an LLM.\n",
    "#    (For example, we use a small FLAN-T5 model from Hugging Face via HuggingFaceHub)\n",
    "# -------------------------------\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-small\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    model_kwargs={\"temperature\": 10.0}\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Query the chatbot.\n",
    "# -------------------------------\n",
    "query = \"what the total count of diseases in the family health insurance plan?\"\n",
    "result = qa_chain.run(query)\n",
    "print(\"Final Chatbot Output:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.35 (from langchain)\n",
      "  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (2.1)\n",
      "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 0.8/1.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n",
      "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.3.11-py3-none-any.whl (335 kB)\n",
      "Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Installing collected packages: orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed langchain-0.3.19 langchain-core-0.3.40 langchain-text-splitters-0.3.6 langsmith-0.3.11 orjson-3.10.15\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.40)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.19)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.11)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.18 marshmallow-3.26.1 pydantic-settings-2.8.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import textrazor\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "textrazor.api_key = \"abcb5bcd144cef1b383b35871a75965da9f9475132a54e0454d5221d\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_plan_names = [\n",
    "    \"individual health insurance\", \"family health insurance\", \"parents health insurance\",\n",
    "    \"protability insurance\", \"children health insurance\", \"accidental coverage\", \"covid coverage\"\n",
    "]\n",
    "allowed_premium_types = [\"basic\", \"lite\", \"premier\"]\n",
    "allowed_diseases = [\n",
    "    # Individual Health Insurance (34)\n",
    "    \"Acute Myocardial Infarction\",\n",
    "    \"Alzheimer's Disease\",\n",
    "    \"Angina Pectoris\",\n",
    "    \"Aorta Surgery\",\n",
    "    \"Aortic Dissection\",\n",
    "    \"Atrial Flutter\",\n",
    "    \"Brain Surgery\",\n",
    "    \"Cancer\",\n",
    "    \"Cardiomyopathy\",\n",
    "    \"Chronic Liver Disease\",\n",
    "    \"Chronic Lung Disease\",\n",
    "    \"Congestive Heart Failure\",\n",
    "    \"Coronary Artery Disease\",\n",
    "    \"Endocarditis\",\n",
    "    \"Heart Attack\",\n",
    "    \"Kidney Failure\",\n",
    "    \"Left Ventricular Hypertrophy\",\n",
    "    \"Major Organ Transplant\",\n",
    "    \"Motor Neuron Disease\",\n",
    "    \"Multiple Sclerosis\",\n",
    "    \"Myocarditis\",\n",
    "    \"Parkinson's Disease\",\n",
    "    \"Pericarditis\",\n",
    "    \"Permanent Blindness\",\n",
    "    \"Permanent Deafness\",\n",
    "    \"Permanent Loss of Speech\",\n",
    "    \"Poliomyelitis\",\n",
    "    \"Primary Pulmonary Arterial Hypertension\",\n",
    "    \"Pulmonary Embolism\",\n",
    "    \"Sepsis\",\n",
    "    \"Severe Coma\",\n",
    "    \"Stroke\",\n",
    "    \"Ventricular Fibrillation\",\n",
    "    \"Ventricular Tachycardia\",\n",
    "    \n",
    "    # Family Health Insurance (142)\n",
    "    \"Accidental Injury\",\n",
    "    \"Acid Reflux\",\n",
    "    \"Acute Myocardial Infarction\",\n",
    "    \"Age-related Macular Degeneration\",\n",
    "    \"Alzheimer's Disease\",\n",
    "    \"Amputation\",\n",
    "    \"Angina Pectoris\",\n",
    "    \"Anxiety Disorders\",\n",
    "    \"Aorta Surgery\",\n",
    "    \"Aortic Dissection\",\n",
    "    \"Appendicitis\",\n",
    "    \"Arteriosclerosis\",\n",
    "    \"Arthritis\",\n",
    "    \"Asthma\",\n",
    "    \"Asymptomatic Covid\",\n",
    "    \"Atrial Fibrillation\",\n",
    "    \"Atrial Flutter\",\n",
    "    \"Attention Deficit Disorder\",\n",
    "    \"Binge Eating Disorder\",\n",
    "    \"Bipolar Disorder\",\n",
    "    \"Blunt Force Trauma\",\n",
    "    \"Brain Surgery\",\n",
    "    \"Bronchiolitis\",\n",
    "    \"Burns\",\n",
    "    \"COPD\",\n",
    "    \"Cancer\",\n",
    "    \"Cardiomyopathy\",\n",
    "    \"Chemical Burns\",\n",
    "    \"Chronic Back Pain\",\n",
    "    \"Chronic Fatigue Syndrome\",\n",
    "    \"Chronic Kidney Disease\",\n",
    "    \"Chronic Liver Disease\",\n",
    "    \"Chronic Lung Disease\",\n",
    "    \"Chronic Pancreatitis\",\n",
    "    \"Chronic Sinusitis\",\n",
    "    \"Compound Fracture\",\n",
    "    \"Concussion\",\n",
    "    \"Congestive Heart Failure\",\n",
    "    \"Contusions\",\n",
    "    \"Coronary Artery Disease\",\n",
    "    \"Coronary Heart Disease\",\n",
    "    \"Covid Cardiac Complications\",\n",
    "    \"Covid Complications\",\n",
    "    \"Covid Endothelial Dysfunction\",\n",
    "    \"Covid Pneumonia\",\n",
    "    \"Covid Reinfection\",\n",
    "    \"Covid Respiratory Distress\",\n",
    "    \"Covid-19\",\n",
    "    \"Covid-19 Associated ARDS\",\n",
    "    \"Covid-19 Induced Cytokine Storm\",\n",
    "    \"Covid-Induced Inflammation\",\n",
    "    \"Covid-Related Coagulopathy\",\n",
    "    \"Covid-Related Myocarditis\",\n",
    "    \"Covid-Related Neurological Syndrome\",\n",
    "    \"Covid-Related Renal Failure\",\n",
    "    \"Croup\",\n",
    "    \"Crush Injury\",\n",
    "    \"Dementia\",\n",
    "    \"Dengue Fever\",\n",
    "    \"Depression\",\n",
    "    \"Diabetes Mellitus\",\n",
    "    \"Diabetic Retinopathy\",\n",
    "    \"Dislocations\",\n",
    "    \"Ear Infections\",\n",
    "    \"Eczema\",\n",
    "    \"Electrical Injury\",\n",
    "    \"Endocarditis\",\n",
    "    \"Fibromyalgia\",\n",
    "    \"Fracture\",\n",
    "    \"Fracture Treatment\",\n",
    "    \"Gastroenteritis\",\n",
    "    \"Glaucoma\",\n",
    "    \"Gout\",\n",
    "    \"Hand, Foot, and Mouth Disease\",\n",
    "    \"Head Injury\",\n",
    "    \"Hearing Loss\",\n",
    "    \"Heart Attack\",\n",
    "    \"Heart Failure\",\n",
    "    \"Heart Valve Disease\",\n",
    "    \"Hyperlipidemia\",\n",
    "    \"Hypertension\",\n",
    "    \"Hypertensive Heart Disease\",\n",
    "    \"Insomnia\",\n",
    "    \"Internal Injury\",\n",
    "    \"Irritable Bowel Syndrome\",\n",
    "    \"Kidney Failure\",\n",
    "    \"Kidney Stones\",\n",
    "    \"Lacerations\",\n",
    "    \"Laryngitis\",\n",
    "    \"Left Ventricular Hypertrophy\",\n",
    "    \"Long Covid\",\n",
    "    \"Major Organ Transplant\",\n",
    "    \"Malaria\",\n",
    "    \"Measles Complications\",\n",
    "    \"Migraines\",\n",
    "    \"Mild Covid\",\n",
    "    \"Motor Neuron Disease\",\n",
    "    \"Multiple Sclerosis\",\n",
    "    \"Mumps Complications\",\n",
    "    \"Myocarditis\",\n",
    "    \"Obesity\",\n",
    "    \"Obsessive-Compulsive Disorder\",\n",
    "    \"Open Fracture\",\n",
    "    \"Osteoarthritis\",\n",
    "    \"Osteoporosis\",\n",
    "    \"Panic Disorder\",\n",
    "    \"Parkinson's Disease\",\n",
    "    \"Penetrating Wound\",\n",
    "    \"Pericarditis\",\n",
    "    \"Peripheral Artery Disease\",\n",
    "    \"Peripheral Neuropathy\",\n",
    "    \"Permanent Blindness\",\n",
    "    \"Permanent Deafness\",\n",
    "    \"Permanent Loss of Speech\",\n",
    "    \"Pertussis\",\n",
    "    \"Poliomyelitis\",\n",
    "    \"Post Covid Syndrome\",\n",
    "    \"Post-Traumatic Stress Disorder\",\n",
    "    \"Post-Viral Fatigue Syndrome\",\n",
    "    \"Primary Pulmonary Arterial Hypertension\",\n",
    "    \"Pulmonary Embolism\",\n",
    "    \"RSV Infection\",\n",
    "    \"Restless Leg Syndrome\",\n",
    "    \"Rheumatoid Arthritis\",\n",
    "    \"Scarlet Fever\",\n",
    "    \"Seasonal Affective Disorder\",\n",
    "    \"Sepsis\",\n",
    "    \"Severe Chickenpox\",\n",
    "    \"Severe Coma\",\n",
    "    \"Severe Covid\",\n",
    "    \"Social Anxiety Disorder\",\n",
    "    \"Soft Tissue Injury\",\n",
    "    \"Spinal Injury\",\n",
    "    \"Strep Throat\",\n",
    "    \"Stroke\",\n",
    "    \"Tension Headaches\",\n",
    "    \"Thyroid Disorders\",\n",
    "    \"Traumatic Brain Injury\",\n",
    "    \"Varicella\",\n",
    "    \"Ventricular Fibrillation\",\n",
    "    \"Ventricular Tachycardia\",\n",
    "    \"Whooping Cough\",\n",
    "    \n",
    "    # Parents Health Insurance (30)\n",
    "    \"Age-related Macular Degeneration\",\n",
    "    \"Arteriosclerosis\",\n",
    "    \"Arthritis\",\n",
    "    \"Atrial Fibrillation\",\n",
    "    \"COPD\",\n",
    "    \"Cancer\",\n",
    "    \"Chronic Back Pain\",\n",
    "    \"Chronic Kidney Disease\",\n",
    "    \"Chronic Pancreatitis\",\n",
    "    \"Coronary Heart Disease\",\n",
    "    \"Dementia\",\n",
    "    \"Diabetes Mellitus\",\n",
    "    \"Diabetic Retinopathy\",\n",
    "    \"Glaucoma\",\n",
    "    \"Gout\",\n",
    "    \"Hearing Loss\",\n",
    "    \"Heart Failure\",\n",
    "    \"Heart Valve Disease\",\n",
    "    \"Hyperlipidemia\",\n",
    "    \"Hypertension\",\n",
    "    \"Hypertensive Heart Disease\",\n",
    "    \"Kidney Stones\",\n",
    "    \"Obesity\",\n",
    "    \"Osteoarthritis\",\n",
    "    \"Osteoporosis\",\n",
    "    \"Peripheral Artery Disease\",\n",
    "    \"Peripheral Neuropathy\",\n",
    "    \"Rheumatoid Arthritis\",\n",
    "    \"Stroke\",\n",
    "    \"Thyroid Disorders\",\n",
    "    \n",
    "    # Protability Insurance (20)\n",
    "    \"Acid Reflux\",\n",
    "    \"Anxiety Disorders\",\n",
    "    \"Attention Deficit Disorder\",\n",
    "    \"Binge Eating Disorder\",\n",
    "    \"Bipolar Disorder\",\n",
    "    \"Chronic Fatigue Syndrome\",\n",
    "    \"Chronic Sinusitis\",\n",
    "    \"Depression\",\n",
    "    \"Eczema\",\n",
    "    \"Fibromyalgia\",\n",
    "    \"Insomnia\",\n",
    "    \"Irritable Bowel Syndrome\",\n",
    "    \"Migraines\",\n",
    "    \"Obsessive-Compulsive Disorder\",\n",
    "    \"Panic Disorder\",\n",
    "    \"Post-Traumatic Stress Disorder\",\n",
    "    \"Restless Leg Syndrome\",\n",
    "    \"Seasonal Affective Disorder\",\n",
    "    \"Social Anxiety Disorder\",\n",
    "    \"Tension Headaches\",\n",
    "    \n",
    "    # Children Health Insurance (20)\n",
    "    \"Appendicitis\",\n",
    "    \"Asthma\",\n",
    "    \"Bronchiolitis\",\n",
    "    \"Croup\",\n",
    "    \"Dengue Fever\",\n",
    "    \"Ear Infections\",\n",
    "    \"Fracture Treatment\",\n",
    "    \"Gastroenteritis\",\n",
    "    \"Hand, Foot, and Mouth Disease\",\n",
    "    \"Laryngitis\",\n",
    "    \"Malaria\",\n",
    "    \"Measles Complications\",\n",
    "    \"Mumps Complications\",\n",
    "    \"Pertussis\",\n",
    "    \"RSV Infection\",\n",
    "    \"Scarlet Fever\",\n",
    "    \"Severe Chickenpox\",\n",
    "    \"Strep Throat\",\n",
    "    \"Varicella\",\n",
    "    \"Whooping Cough\",\n",
    "    \n",
    "    # Accidental Coverage (20)\n",
    "    \"Accidental Injury\",\n",
    "    \"Amputation\",\n",
    "    \"Blunt Force Trauma\",\n",
    "    \"Burns\",\n",
    "    \"Chemical Burns\",\n",
    "    \"Compound Fracture\",\n",
    "    \"Concussion\",\n",
    "    \"Contusions\",\n",
    "    \"Crush Injury\",\n",
    "    \"Dislocations\",\n",
    "    \"Electrical Injury\",\n",
    "    \"Fracture\",\n",
    "    \"Head Injury\",\n",
    "    \"Internal Injury\",\n",
    "    \"Lacerations\",\n",
    "    \"Open Fracture\",\n",
    "    \"Penetrating Wound\",\n",
    "    \"Soft Tissue Injury\",\n",
    "    \"Spinal Injury\",\n",
    "    \"Traumatic Brain Injury\",\n",
    "    \n",
    "    # Covid Coverage (20)\n",
    "    \"Asymptomatic Covid\",\n",
    "    \"Covid Cardiac Complications\",\n",
    "    \"Covid Complications\",\n",
    "    \"Covid Endothelial Dysfunction\",\n",
    "    \"Covid Pneumonia\",\n",
    "    \"Covid Reinfection\",\n",
    "    \"Covid Respiratory Distress\",\n",
    "    \"Covid-19\",\n",
    "    \"Covid-19 Associated ARDS\",\n",
    "    \"Covid-19 Induced Cytokine Storm\",\n",
    "    \"Covid-Induced Inflammation\",\n",
    "    \"Covid-Related Coagulopathy\",\n",
    "    \"Covid-Related Myocarditis\",\n",
    "    \"Covid-Related Neurological Syndrome\",\n",
    "    \"Covid-Related Renal Failure\",\n",
    "    \"Long Covid\",\n",
    "    \"Mild Covid\",\n",
    "    \"Post Covid Syndrome\",\n",
    "    \"Post-Viral Fatigue Syndrome\",\n",
    "    \"Severe Covid\"\n",
    "]\n",
    "\n",
    "# Premium percentages for Level 5 calculations.\n",
    "premium_percentages = {\n",
    "    \"Basic\": {\"deductible\": 5, \"copay\": 20},\n",
    "    \"Lite\": {\"deductible\": 10, \"copay\": 10},\n",
    "    \"Premier\": {\"deductible\": 15, \"copay\": 5},\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved as 'dataset\\knowlede_plan_disease_combinations_updated.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def get_plan_payment_range(plan, premium):\n",
    "    \"\"\"\n",
    "    Returns the (min, max) monthly payment range for a given plan and premium type,\n",
    "    based on the new business rules.\n",
    "    \"\"\"\n",
    "    if plan == \"Individual Health Insurance\":\n",
    "        if premium == \"Basic\":\n",
    "            return (2000, 10000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (10000, 15000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (15000, 20000)\n",
    "    elif plan == \"Family Health Insurance\":\n",
    "        if premium == \"Basic\":\n",
    "            return (5000, 12000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (12000, 17000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (17000, 25000)\n",
    "    elif plan == \"Parents Health Insurance\":\n",
    "        if premium == \"Basic\":\n",
    "            return (2000, 10000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (10000, 15000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (15000, 20000)\n",
    "    elif plan == \"Protability Insurance\":\n",
    "        if premium == \"Basic\":\n",
    "            return (2000, 10000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (10000, 15000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (15000, 20000)\n",
    "    elif plan == \"Accidental Coverage\":\n",
    "        if premium == \"Basic\":\n",
    "            return (2000, 10000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (10000, 15000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (15000, 20000)\n",
    "    elif plan == \"Covid Coverage\":\n",
    "        if premium == \"Basic\":\n",
    "            return (2000, 10000)\n",
    "        elif premium == \"Lite\":\n",
    "            return (10000, 15000)\n",
    "        elif premium == \"Premier\":\n",
    "            return (15000, 20000)\n",
    "    elif plan == \"Children Health Insurance\":\n",
    "        if premium == \"Basic\":\n",
    "            return (240, 500)\n",
    "        elif premium == \"Lite\":\n",
    "            return (320, 700)\n",
    "        elif premium == \"Premier\":\n",
    "            return (400, 900)\n",
    "    else:\n",
    "        return (2000, 10000)\n",
    "\n",
    "def generate_new_monthly_payment(row):\n",
    "    plan = row[\"Plan Name\"]\n",
    "    premium = row[\"Premium Type\"]\n",
    "    rng = get_plan_payment_range(plan, premium)\n",
    "    return random.randint(rng[0], rng[1])\n",
    "\n",
    "# Read the CSV file (adjust the file path as needed)\n",
    "df = pd.read_csv(r\"dataset\\knowlede_plan_disease_combinations.csv\")\n",
    "\n",
    "# Update the \"Monthly Payment\" column using our new business rules\n",
    "df[\"Monthly Payment\"] = df.apply(generate_new_monthly_payment, axis=1)\n",
    "\n",
    "# (Optional) If needed, you could also update related columns such as \"Deductible\" or \"Benefits\"\n",
    "# to reflect the new payment structure.\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(r\"dataset\\knowlede_plan_disease_combinations_updated.csv\", index=False)\n",
    "\n",
    "print(\"Updated CSV file saved as 'dataset\\\\knowlede_plan_disease_combinations_updated.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed candidate lists and target columns (DO NOT CHANGE THESE)\n",
    "allowed_plan_names = [\n",
    "    \"individual health insurance\", \"family health insurance\", \"parents health insurance\",\n",
    "    \"protability insurance\", \"children health insurance\", \"accidental coverage\", \"covid coverage\"\n",
    "]\n",
    "allowed_premium_types = [\"basic\", \"lite\", \"premier\"]\n",
    "\n",
    "# For Level 4, allowed targets:\n",
    "level4_allowed = [\"Maximum Coverage\", \"Deductible\", \"Co-pay Percentage\", \"Waiting Period\", \n",
    "                  \"Claims Settled\", \"Renewability\", \"Hospital Coverage\", \"Benefits\", \"Tax Redemption\"]\n",
    "# For Level 5, allowed targets are Level 4's plus \"Calculation\"\n",
    "allowed_target_columns_by_level = {\n",
    "    1: [\"Premium Type\", \"DisCount\", \"Diseases\", \"CoverageLevel\", \"PlanFocus\", \"Advantage\"],\n",
    "    2: [\"Plan Name\", \"Deductible\", \"Co-pay Percentage\", \"Plan Term\", \"Tax Redemption\", \"Plan Count\", \"Monthly Payment\", \"Advantage\"],\n",
    "    3: [\"Maximum Coverage\", \"Deductible\", \"Co-pay Percentage\", \"Waiting Period\", \"Claims Settled\", \"Renewability\", \"Hospital Coverage\", \"Benefits\", \"Tax Redemption\"],\n",
    "    4: level4_allowed,\n",
    "    5: [\"Calculation\"] + level4_allowed\n",
    "}\n",
    "\n",
    "synonyms_mapping = {\n",
    "    \"Premium Type\": [\"premium\", \"premium type\", \"plan premium\", \"insurance premium\", \"option premium\", \"type of premium\",\n",
    "                     \"available premium\", \"selected premium\", \"premium category\", \"premium option\"],\n",
    "    \"DisCount\": [\"disease count\", \"no of diseases\", \"number of diseases\", \"total diseases\"],\n",
    "    \"Diseases\": [\"diseases\", \"illnesses\", \"conditions\", \"health issues\", \"medical conditions\", \"afflictions\",\n",
    "                 \"sicknesses\", \"disorders\", \"ailments\", \"pathologies\", \"diseases covered\"],\n",
    "    \"CoverageLevel\": [\"coverage level\", \"plan coverage\", \"protection level\", \"coverage rating\", \"extent of coverage\",\n",
    "                      \"coverage extent\", \"protection rating\", \"insurance level\", \"coverage standard\", \"coverage benchmark\", \"total coverage\"],\n",
    "    \"PlanFocus\": [\"plan focus\", \"specialization\", \"focus area\", \"core focus\", \"primary focus\", \"key focus\",\n",
    "                  \"service focus\", \"area of expertise\", \"focus field\", \"target focus\"],\n",
    "    \"Advantage\": [\"advantage\", \"advantages\", \"benefits\", \"perks\", \"features\", \"positive aspects\", \"merits\", \"edge\",\n",
    "                  \"plus points\", \"competitive advantage\"],\n",
    "    \"Plan Name\": [\"plan name\", \"insurance plan\", \"policy name\", \"name of the plan\", \"plan title\", \"plan designation\",\n",
    "                  \"title of the plan\", \"name\", \"plan identifier\", \"plan\"],\n",
    "    \"Deductible\": [\"deductible\", \"deductibles\", \"deduction\", \"amount deducted\", \"deductible amount\", \"deduction amount\",\n",
    "                   \"deduct\", \"cost reduction\", \"expense reduction\"],\n",
    "    \"Co-pay Percentage\": [\"co-pay percentage\", \"copay\", \"co payment\", \"co-payment\", \"co-pay rate\", \"copayment percentage\",\n",
    "                          \"co-pay amount\", \"percentage for copay\", \"cost sharing percentage\", \"co-payment rate\"],\n",
    "    \"Plan Term\": [\"plan term\", \"duration\", \"coverage period\", \"plan duration\", \"term of coverage\", \"length of plan\",\n",
    "                  \"insurance term\", \"policy duration\", \"term\", \"contract term\"],\n",
    "    \"Tax Redemption\": [\"tax redemption\", \"tax benefits\", \"tax return\", \"tax reclaim\", \"tax refund\", \"tax savings\",\n",
    "                       \"tax incentive\", \"tax deduction\", \"tax relief\", \"tax advantage\"],\n",
    "    \"Plan Count\": [\"plan count\", \"number of plans\", \"total plans\", \"plans available\", \"policy count\", \"plan quantity\",\n",
    "                   \"number of options\", \"available plans\", \"plan total\", \"plan inventory\"],\n",
    "    \"Monthly Payment\": [\"monthly payment\", \"monthly cost\", \"cost per month\", \"payment per month\", \"monthly premium\",\n",
    "                        \"monthly charge\", \"recurring monthly expense\", \"monthly installment\", \"month-to-month payment\",\n",
    "                        \"monthly fee\"],\n",
    "    \"Benefits\": [\"benefits\", \"list benefits\", \"explain benefits\", \"advantages\", \"what are the perks\",\n",
    "                 \"feature list\", \"positive aspects\", \"the benefits\", \"key benefits\", \"benefit details\"],\n",
    "    \"Maximum Coverage\": [\"maximum coverage\", \"max coverage\", \"coverage limit\", \"limit\", \"coverage cap\", \"maximum limit\",\n",
    "                         \"coverage maximum\", \"max insured amount\", \"cap\", \"maximum protection\"],\n",
    "    \"Waiting Period\": [\"waiting period\", \"delay before coverage\", \"coverage waiting time\", \"time before coverage\",\n",
    "                       \"waiting time\", \"period before benefits\", \"initiation delay\", \"coverage delay\", \"activation time\",\n",
    "                       \"delay period\"],\n",
    "    \"Claims Settled\": [\"claims settled\", \"number of claims\", \"settled claims\", \"claim count\", \"claims resolved\",\n",
    "                       \"settled claim number\", \"resolved claims\", \"claim settlement\", \"claims processed\", \"total claims\"],\n",
    "    \"Renewability\": [\"renewability\", \"renewal\", \"can be renewed\", \"renewable\", \"renewal terms\", \"renewal period\",\n",
    "                     \"term extension\", \"renewable status\", \"renewal option\", \"renewal criteria\"],\n",
    "    \"Hospital Coverage\": [\"hospital coverage\", \"inpatient coverage\", \"hospitalization coverage\", \"hospital care\",\n",
    "                          \"coverage for hospitalization\", \"hospital benefits\", \"inpatient benefits\", \"hospital services coverage\",\n",
    "                          \"hospital claim\", \"hospital expense\"],\n",
    "    \"Calculation\": [\"calculation\", \"compute cost\", \"out-of-pocket\", \"final expense\", \"total cost\", \"claim calculation\",\n",
    "                    \"expense calculation\", \"cost breakdown\", \"calculate expense\", \"final cost\"]\n",
    "}\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Generate n-grams from a list of tokens.\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def analyze_text(query):\n",
    "    client = textrazor.TextRazor(extractors=[\"entities\"])\n",
    "    response = client.analyze(query)\n",
    "    entities_info = []\n",
    "    for entity in response.entities():\n",
    "        entities_info.append({\n",
    "            \"entity\": entity.id,\n",
    "            \"relevance\": entity.relevance_score,\n",
    "            \"freebase_types\": entity.freebase_types\n",
    "        })\n",
    "    return entities_info\n",
    "\n",
    "def extract_entities_from_textrazor(query):\n",
    "    entities = analyze_text(query)\n",
    "    extraction = {}\n",
    "    for ent in entities:\n",
    "        text = ent[\"entity\"].lower().strip()\n",
    "        for plan in allowed_plan_names:\n",
    "            if plan.lower() in text:\n",
    "                extraction[\"Plan Name\"] = plan.title()\n",
    "        for premium in allowed_premium_types:\n",
    "            if re.search(r\"\\b\" + re.escape(premium.lower()) + r\"\\b\", text):\n",
    "                extraction[\"Premium Type\"] = premium.title()\n",
    "        for disease in allowed_diseases:\n",
    "            if disease.lower() in text:\n",
    "                extraction[\"Disease\"] = disease\n",
    "        if re.fullmatch(r\"\\d+\", text):\n",
    "            extraction[\"Amount\"] = int(text)\n",
    "    return extraction\n",
    "\n",
    "def extract_entities_forcefully(query):\n",
    "    extraction = {}\n",
    "    lower_query = query.lower()\n",
    "    for plan in allowed_plan_names:\n",
    "        if plan.lower() in lower_query:\n",
    "            extraction[\"Plan Name\"] = plan.title()\n",
    "            break\n",
    "    for premium in allowed_premium_types:\n",
    "        if re.search(r\"\\b\" + re.escape(premium.lower()) + r\"\\b\", lower_query):\n",
    "            extraction[\"Premium Type\"] = premium.title()\n",
    "            break\n",
    "    for disease in allowed_diseases:\n",
    "        if disease.lower() in lower_query:\n",
    "            extraction[\"Disease\"] = disease\n",
    "            break\n",
    "    digit_sequences = re.findall(r\"\\d+\", lower_query)\n",
    "    if digit_sequences:\n",
    "        longest = max(digit_sequences, key=len)\n",
    "        extraction[\"Amount\"] = int(longest)\n",
    "    return extraction\n",
    "\n",
    "def extract_entities(query):\n",
    "    tex_extraction = extract_entities_from_textrazor(query)\n",
    "    forced_extraction = extract_entities_forcefully(query)\n",
    "    combined = {**tex_extraction, **forced_extraction}\n",
    "    return combined\n",
    "\n",
    "def determine_level(extraction):\n",
    "    if \"Amount\" in extraction:\n",
    "        return 5\n",
    "    if \"Plan Name\" in extraction and \"Premium Type\" in extraction and \"Disease\" in extraction:\n",
    "        return 4\n",
    "    if \"Plan Name\" in extraction and \"Premium Type\" in extraction:\n",
    "        return 3\n",
    "    if \"Premium Type\" in extraction:\n",
    "        return 2\n",
    "    if \"Plan Name\" in extraction:\n",
    "        return 1\n",
    "    return 1\n",
    "\n",
    "def infer_target_columns(level, query, extraction):\n",
    "    allowed_targets = set(allowed_target_columns_by_level.get(level, []))\n",
    "    lower_query = query.lower()\n",
    "    inferred = set()\n",
    "    tokens = re.findall(r\"\\w+\", lower_query)\n",
    "    \n",
    "    # First, try matching 2-grams (bigrams)\n",
    "    bigrams = generate_ngrams(tokens, 2)\n",
    "    for target, syn_list in synonyms_mapping.items():\n",
    "        if target in {\"Plan Name\", \"Premium Type\", \"Diseases\"}:\n",
    "            continue\n",
    "        for syn in syn_list:\n",
    "            for bg in bigrams:\n",
    "                if fuzz.ratio(bg, syn.lower()) >= 70:\n",
    "                    inferred.add(target)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # If nothing inferred from bigrams, fall back to 1-gram matching.\n",
    "    if not inferred:\n",
    "        for target, syn_list in synonyms_mapping.items():\n",
    "            if target in {\"Plan Name\", \"Premium Type\", \"Diseases\"}:\n",
    "                continue\n",
    "            for syn in syn_list:\n",
    "                for token in tokens:\n",
    "                    if fuzz.ratio(token, syn.lower()) >= 85:\n",
    "                        inferred.add(target)\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "    # For Level 5, if query mentions \"cover\" or \"how much\", add Calculation explicitly.\n",
    "    if level == 5 and \"Amount\" in extraction:\n",
    "        if \"cover\" in lower_query or \"how much\" in lower_query:\n",
    "            inferred.add(\"Calculation\")\n",
    "    inferred = inferred.intersection(allowed_targets)\n",
    "    if not inferred:\n",
    "        inferred = allowed_targets\n",
    "    return list(inferred)\n",
    "\n",
    "def generate_final_answer(query):\n",
    "    extraction = extract_entities(query)\n",
    "    level = determine_level(extraction)\n",
    "    extraction[\"Determined Level\"] = level\n",
    "    input_sections = {\n",
    "        1: [\"Plan Name\"],\n",
    "        2: [\"Premium Type\"],\n",
    "        3: [\"Plan Name\", \"Premium Type\"],\n",
    "        4: [\"Plan Name\", \"Disease\", \"Premium Type\"],\n",
    "        5: [\"Plan Name\", \"Disease\", \"Premium Type\", \"Amount\"]\n",
    "    }\n",
    "    extraction[\"Input Sections\"] = input_sections.get(level, [])\n",
    "    inferred_targets = infer_target_columns(level, query, extraction)\n",
    "    extraction[\"Mapped Target Columns\"] = inferred_targets\n",
    "    extraction[\"Allowed Target Columns\"] = allowed_target_columns_by_level.get(level, [])\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def load_docs_from_csv(csv_path):\n",
    "    loader = CSVLoader(file_path=csv_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "csv_paths = [\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_plan_grouping.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_premium_grouping.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowlede_plan_disease_combinations.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\insurance_plans_by_disease.csv'\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for path in csv_paths:\n",
    "    all_docs.extend(load_docs_from_csv(path))\n",
    "\n",
    "# Use the updated embeddings import\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "combined_vectorstore = FAISS.from_documents(all_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "retriever = combined_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-small\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query:\n",
      "what is the total disease covered in the individual health insurance plan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lathe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Chain Output:\n",
      "(4).\n",
      "\n",
      "Rule-Based Extraction Result:\n",
      "{'Plan Name': 'Individual Health Insurance', 'Determined Level': 1, 'Input Sections': ['Plan Name'], 'Mapped Target Columns': ['DisCount', 'CoverageLevel'], 'Allowed Target Columns': ['Premium Type', 'DisCount', 'Diseases', 'CoverageLevel', 'PlanFocus', 'Advantage']}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_level1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m temp_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m record \u001b[38;5;241m=\u001b[39m retrieve_record(extraction_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetermined Level\u001b[39m\u001b[38;5;124m\"\u001b[39m], extraction_result)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNo matching record found in the datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mretrieve_record\u001b[1;34m(level, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_record\u001b[39m(level, inputs):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Try each CSV dataset in order; return the first match.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m [df_level1, df_level2, df_level3, df_level4]:\n\u001b[0;32m     19\u001b[0m         temp_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_level1' is not defined"
     ]
    }
   ],
   "source": [
    "sample_query = \"what is the total disease covered in the individual health insurance plan?\"\n",
    "print(\"Sample Query:\")\n",
    "print(sample_query)\n",
    "\n",
    "# 1. Run the RAG chain on the combined vector store.\n",
    "rag_result = qa_chain.run(sample_query)\n",
    "print(\"\\nRAG Chain Output:\")\n",
    "print(rag_result)\n",
    "\n",
    "# 2. Run rule-based extraction for the query.\n",
    "extraction_result = generate_final_answer(sample_query)\n",
    "print(\"\\nRule-Based Extraction Result:\")\n",
    "print(extraction_result)\n",
    "\n",
    "# 3. Retrieve a record from the knowledge datasets (try each CSV until one matches).\n",
    "def retrieve_record(level, inputs):\n",
    "    for df in [df_level1, df_level2, df_level3, df_level4]:\n",
    "        temp_df = df.copy()\n",
    "        for key, value in inputs.items():\n",
    "            if key in temp_df.columns:\n",
    "                temp_df = temp_df[temp_df[key].str.contains(value, case=False, na=False)]\n",
    "        if not temp_df.empty:\n",
    "            return temp_df.iloc[0].to_dict()\n",
    "    return None\n",
    "\n",
    "record = retrieve_record(extraction_result[\"Determined Level\"], extraction_result)\n",
    "if record is None:\n",
    "    print(\"\\nNo matching record found in the datasets.\")\n",
    "else:\n",
    "    print(\"\\nRetrieved Record from Datasets:\")\n",
    "    print(record)\n",
    "    \n",
    "    # Build final mapping from input tokens to output target columns.\n",
    "    def compute_out_of_pocket(record, amount, premium_type):\n",
    "        premium_type = premium_type.lower()\n",
    "        if \"basic\" in premium_type:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        elif \"lite\" in premium_type:\n",
    "            deductible_pct = 10\n",
    "            copay_pct = 10\n",
    "        elif \"premier\" in premium_type:\n",
    "            deductible_pct = 15\n",
    "            copay_pct = 5\n",
    "        else:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        try:\n",
    "            max_coverage = float(record.get(\"Maximum Coverage\", 0))\n",
    "        except:\n",
    "            max_coverage = 0\n",
    "        if max_coverage > 0 and amount > max_coverage:\n",
    "            return f\"Claim amount {amount} exceeds maximum coverage of {max_coverage}.\"\n",
    "        deductible_value = amount * deductible_pct / 100\n",
    "        updated_amount = amount - deductible_value\n",
    "        copay_value = updated_amount * copay_pct / 100\n",
    "        final_out_of_pocket = deductible_value + copay_value\n",
    "        return final_out_of_pocket\n",
    "\n",
    "    output_mapping = {}\n",
    "    for col in extraction_result[\"Allowed Target Columns\"]:\n",
    "        if col == \"Calculation\":\n",
    "            if \"Amount\" in extraction_result and \"Premium Type\" in extraction_result:\n",
    "                output_mapping[col] = compute_out_of_pocket(record, extraction_result[\"Amount\"], extraction_result[\"Premium Type\"])\n",
    "            else:\n",
    "                output_mapping[col] = None\n",
    "        else:\n",
    "            output_mapping[col] = record.get(col, None)\n",
    "    \n",
    "    final_mapping = {\n",
    "        \"Input Tokens\": {k: extraction_result[k] for k in extraction_result if k in [\"Plan Name\", \"Premium Type\", \"Disease\", \"Amount\"]},\n",
    "        \"Output Target Columns\": output_mapping\n",
    "    }\n",
    "    print(\"\\nFinal Mapping (Input Tokens -> Output Target Columns):\")\n",
    "    print(final_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.29.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.3.40)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-huggingface) (3.4.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.21.0)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-huggingface) (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.11.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.8.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lathe\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n",
      "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import textrazor\n",
    "\n",
    "# LangChain and related imports\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # updated import from langchain-huggingface package\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Set environment variables for tokens\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "textrazor.api_key = \"abcb5bcd144cef1b383b35871a75965da9f9475132a54e0454d5221d\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV files for each level\n",
    "# -------------------------------\n",
    "# Replace the paths below with the correct paths on your system.\n",
    "df_level1 = pd.read_csv(r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_plan_grouping.csv')\n",
    "df_level2 = pd.read_csv(r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_premium_grouping.csv')\n",
    "df_level3 = pd.read_csv(r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowlede_plan_disease_combinations.csv')\n",
    "df_level4 = pd.read_csv(r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\insurance_plans_by_disease.csv')\n",
    "\n",
    "# -------------------------------\n",
    "# Rule-based Extraction Functions (with 2-gram logic)\n",
    "# -------------------------------\n",
    "allowed_plan_names = [\n",
    "    \"individual health insurance\", \"family health insurance\", \"parents health insurance\",\n",
    "    \"protability insurance\", \"children health insurance\", \"accidental coverage\", \"covid coverage\"\n",
    "]\n",
    "allowed_premium_types = [\"basic\", \"lite\", \"premier\"]\n",
    "\n",
    "level4_allowed = [\"Maximum Coverage\", \"Deductible\", \"Co-pay Percentage\", \"Waiting Period\", \n",
    "                  \"Claims Settled\", \"Renewability\", \"Hospital Coverage\", \"Benefits\", \"Tax Redemption\"]\n",
    "\n",
    "allowed_target_columns_by_level = {\n",
    "    1: [\"Premium Type\", \"DisCount\", \"Diseases\", \"CoverageLevel\", \"PlanFocus\", \"Advantage\"],\n",
    "    2: [\"Plan Name\", \"Deductible\", \"Co-pay Percentage\", \"Plan Term\", \"Tax Redemption\", \"Plan Count\", \"Monthly Payment\", \"Advantage\"],\n",
    "    3: [\"Maximum Coverage\", \"Deductible\", \"Co-pay Percentage\", \"Waiting Period\", \"Claims Settled\", \"Renewability\", \"Hospital Coverage\", \"Benefits\", \"Tax Redemption\"],\n",
    "    4: level4_allowed,\n",
    "    5: [\"Calculation\"] + level4_allowed\n",
    "}\n",
    "\n",
    "synonyms_mapping = {\n",
    "    \"Premium Type\": [\"premium\", \"premium type\", \"plan premium\", \"insurance premium\", \"option premium\", \"type of premium\",\n",
    "                     \"available premium\", \"selected premium\", \"premium category\", \"premium option\"],\n",
    "    \"DisCount\": [\"disease count\", \"no of diseases\", \"number of diseases\", \"total diseases\"],\n",
    "    \"Diseases\": [\"diseases\", \"illnesses\", \"conditions\", \"health issues\", \"medical conditions\", \"afflictions\",\n",
    "                 \"sicknesses\", \"disorders\", \"ailments\", \"pathologies\", \"diseases covered\"],\n",
    "    \"CoverageLevel\": [\"coverage level\", \"plan coverage\", \"protection level\", \"coverage rating\", \"extent of coverage\",\n",
    "                      \"coverage extent\", \"protection rating\", \"insurance level\", \"coverage standard\", \"coverage benchmark\", \"total coverage\"],\n",
    "    \"PlanFocus\": [\"plan focus\", \"specialization\", \"focus area\", \"core focus\", \"primary focus\", \"key focus\",\n",
    "                  \"service focus\", \"area of expertise\", \"focus field\", \"target focus\"],\n",
    "    \"Advantage\": [\"advantage\", \"advantages\", \"benefits\", \"perks\", \"features\", \"positive aspects\", \"merits\", \"edge\",\n",
    "                  \"plus points\", \"competitive advantage\"],\n",
    "    \"Plan Name\": [\"plan name\", \"insurance plan\", \"policy name\", \"name of the plan\", \"plan title\", \"plan designation\",\n",
    "                  \"title of the plan\", \"name\", \"plan identifier\", \"plan\"],\n",
    "    \"Deductible\": [\"deductible\", \"deductibles\", \"deduction\", \"amount deducted\", \"deductible amount\", \"deduction amount\",\n",
    "                   \"deduct\", \"cost reduction\", \"expense reduction\"],\n",
    "    \"Co-pay Percentage\": [\"co-pay percentage\", \"copay\", \"co payment\", \"co-payment\", \"co-pay rate\", \"copayment percentage\",\n",
    "                          \"co-pay amount\", \"percentage for copay\", \"cost sharing percentage\", \"co-payment rate\"],\n",
    "    \"Plan Term\": [\"plan term\", \"duration\", \"coverage period\", \"plan duration\", \"term of coverage\", \"length of plan\",\n",
    "                  \"insurance term\", \"policy duration\", \"term\", \"contract term\"],\n",
    "    \"Tax Redemption\": [\"tax redemption\", \"tax benefits\", \"tax return\", \"tax reclaim\", \"tax refund\", \"tax savings\",\n",
    "                       \"tax incentive\", \"tax deduction\", \"tax relief\", \"tax advantage\"],\n",
    "    \"Plan Count\": [\"plan count\", \"number of plans\", \"total plans\", \"plans available\", \"policy count\", \"plan quantity\",\n",
    "                   \"number of options\", \"available plans\", \"plan total\", \"plan inventory\"],\n",
    "    \"Monthly Payment\": [\"monthly payment\", \"monthly cost\", \"cost per month\", \"payment per month\", \"monthly premium\",\n",
    "                        \"monthly charge\", \"recurring monthly expense\", \"monthly installment\", \"month-to-month payment\",\n",
    "                        \"monthly fee\"],\n",
    "    \"Benefits\": [\"benefits\", \"list benefits\", \"explain benefits\", \"advantages\", \"what are the perks\",\n",
    "                 \"feature list\", \"positive aspects\", \"the benefits\", \"key benefits\", \"benefit details\"],\n",
    "    \"Maximum Coverage\": [\"maximum coverage\", \"max coverage\", \"coverage limit\", \"limit\", \"coverage cap\", \"maximum limit\",\n",
    "                         \"coverage maximum\", \"max insured amount\", \"cap\", \"maximum protection\"],\n",
    "    \"Waiting Period\": [\"waiting period\", \"delay before coverage\", \"coverage waiting time\", \"time before coverage\",\n",
    "                       \"waiting time\", \"period before benefits\", \"initiation delay\", \"coverage delay\", \"activation time\",\n",
    "                       \"delay period\"],\n",
    "    \"Claims Settled\": [\"claims settled\", \"number of claims\", \"settled claims\", \"claim count\", \"claims resolved\",\n",
    "                       \"settled claim number\", \"resolved claims\", \"claim settlement\", \"claims processed\", \"total claims\"],\n",
    "    \"Renewability\": [\"renewability\", \"renewal\", \"can be renewed\", \"renewable\", \"renewal terms\", \"renewal period\",\n",
    "                     \"term extension\", \"renewable status\", \"renewal option\", \"renewal criteria\"],\n",
    "    \"Hospital Coverage\": [\"hospital coverage\", \"inpatient coverage\", \"hospitalization coverage\", \"hospital care\",\n",
    "                          \"coverage for hospitalization\", \"hospital benefits\", \"inpatient benefits\", \"hospital services coverage\",\n",
    "                          \"hospital claim\", \"hospital expense\"],\n",
    "    \"Calculation\": [\"calculation\", \"compute cost\", \"out-of-pocket\", \"final expense\", \"total cost\", \"claim calculation\",\n",
    "                    \"expense calculation\", \"cost breakdown\", \"calculate expense\", \"final cost\"]\n",
    "}\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Generate n-grams from a list of tokens.\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def analyze_text(query):\n",
    "    client = textrazor.TextRazor(extractors=[\"entities\"])\n",
    "    response = client.analyze(query)\n",
    "    entities_info = []\n",
    "    for entity in response.entities():\n",
    "        entities_info.append({\n",
    "            \"entity\": entity.id,\n",
    "            \"relevance\": entity.relevance_score,\n",
    "            \"freebase_types\": entity.freebase_types\n",
    "        })\n",
    "    return entities_info\n",
    "\n",
    "def extract_entities_from_textrazor(query):\n",
    "    entities = analyze_text(query)\n",
    "    extraction = {}\n",
    "    for ent in entities:\n",
    "        text = ent[\"entity\"].lower().strip()\n",
    "        for plan in allowed_plan_names:\n",
    "            if plan.lower() in text:\n",
    "                extraction[\"Plan Name\"] = plan.title()\n",
    "        for premium in allowed_premium_types:\n",
    "            if re.search(r\"\\b\" + re.escape(premium.lower()) + r\"\\b\", text):\n",
    "                extraction[\"Premium Type\"] = premium.title()\n",
    "        for disease in allowed_diseases:\n",
    "            if disease.lower() in text:\n",
    "                extraction[\"Disease\"] = disease\n",
    "        if re.fullmatch(r\"\\d+\", text):\n",
    "            extraction[\"Amount\"] = int(text)\n",
    "    return extraction\n",
    "\n",
    "def extract_entities_forcefully(query):\n",
    "    extraction = {}\n",
    "    lower_query = query.lower()\n",
    "    for plan in allowed_plan_names:\n",
    "        if plan.lower() in lower_query:\n",
    "            extraction[\"Plan Name\"] = plan.title()\n",
    "            break\n",
    "    for premium in allowed_premium_types:\n",
    "        if re.search(r\"\\b\" + re.escape(premium.lower()) + r\"\\b\", lower_query):\n",
    "            extraction[\"Premium Type\"] = premium.title()\n",
    "            break\n",
    "    for disease in allowed_diseases:\n",
    "        if disease.lower() in lower_query:\n",
    "            extraction[\"Disease\"] = disease\n",
    "            break\n",
    "    digit_sequences = re.findall(r\"\\d+\", lower_query)\n",
    "    if digit_sequences:\n",
    "        longest = max(digit_sequences, key=len)\n",
    "        extraction[\"Amount\"] = int(longest)\n",
    "    return extraction\n",
    "\n",
    "def extract_entities(query):\n",
    "    tex_extraction = extract_entities_from_textrazor(query)\n",
    "    forced_extraction = extract_entities_forcefully(query)\n",
    "    combined = {**tex_extraction, **forced_extraction}\n",
    "    return combined\n",
    "\n",
    "def determine_level(extraction):\n",
    "    if \"Amount\" in extraction:\n",
    "        return 5\n",
    "    if \"Plan Name\" in extraction and \"Premium Type\" in extraction and \"Disease\" in extraction:\n",
    "        return 4\n",
    "    if \"Plan Name\" in extraction and \"Premium Type\" in extraction:\n",
    "        return 3\n",
    "    if \"Premium Type\" in extraction:\n",
    "        return 2\n",
    "    if \"Plan Name\" in extraction:\n",
    "        return 1\n",
    "    return 1\n",
    "\n",
    "def infer_target_columns(level, query, extraction):\n",
    "    allowed_targets = set(allowed_target_columns_by_level.get(level, []))\n",
    "    lower_query = query.lower()\n",
    "    inferred = set()\n",
    "    tokens = re.findall(r\"\\w+\", lower_query)\n",
    "    \n",
    "    # Try matching 2-grams first\n",
    "    bigrams = generate_ngrams(tokens, 2)\n",
    "    for target, syn_list in synonyms_mapping.items():\n",
    "        if target in {\"Plan Name\", \"Premium Type\", \"Diseases\"}:\n",
    "            continue\n",
    "        for syn in syn_list:\n",
    "            for bg in bigrams:\n",
    "                if fuzz.ratio(bg, syn.lower()) >= 70:\n",
    "                    inferred.add(target)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # If no bigram match, try 1-gram fuzzy matching with threshold 85%\n",
    "    if not inferred:\n",
    "        for target, syn_list in synonyms_mapping.items():\n",
    "            if target in {\"Plan Name\", \"Premium Type\", \"Diseases\"}:\n",
    "                continue\n",
    "            for syn in syn_list:\n",
    "                for token in tokens:\n",
    "                    if fuzz.ratio(token, syn.lower()) >= 85:\n",
    "                        inferred.add(target)\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "    # For Level 5, if query mentions \"cover\" or \"how much\", add Calculation explicitly.\n",
    "    if level == 5 and \"Amount\" in extraction:\n",
    "        if \"cover\" in lower_query or \"how much\" in lower_query:\n",
    "            inferred.add(\"Calculation\")\n",
    "    inferred = inferred.intersection(allowed_targets)\n",
    "    if not inferred:\n",
    "        inferred = allowed_targets\n",
    "    return list(inferred)\n",
    "\n",
    "def generate_final_answer(query):\n",
    "    extraction = extract_entities(query)\n",
    "    level = determine_level(extraction)\n",
    "    extraction[\"Determined Level\"] = level\n",
    "    input_sections = {\n",
    "        1: [\"Plan Name\"],\n",
    "        2: [\"Premium Type\"],\n",
    "        3: [\"Plan Name\", \"Premium Type\"],\n",
    "        4: [\"Plan Name\", \"Disease\", \"Premium Type\"],\n",
    "        5: [\"Plan Name\", \"Disease\", \"Premium Type\", \"Amount\"]\n",
    "    }\n",
    "    extraction[\"Input Sections\"] = input_sections.get(level, [])\n",
    "    inferred_targets = infer_target_columns(level, query, extraction)\n",
    "    extraction[\"Mapped Target Columns\"] = inferred_targets\n",
    "    extraction[\"Allowed Target Columns\"] = allowed_target_columns_by_level.get(level, [])\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def load_docs_from_csv(csv_path):\n",
    "    loader = CSVLoader(file_path=csv_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "csv_paths = [\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_plan_grouping.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowledge_premium_grouping.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\knowlede_plan_disease_combinations.csv',\n",
    "    r'C:\\Users\\lathe\\Desktop\\rag model training\\dataset\\insurance_plans_by_disease.csv'\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for path in csv_paths:\n",
    "    all_docs.extend(load_docs_from_csv(path))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "combined_vectorstore = FAISS.from_documents(all_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "retriever = combined_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-small\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query:\n",
      "what is the total disease covered in the individual health insurance plan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lathe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Chain Output:\n",
      "(4).\n",
      "\n",
      "Rule-Based Extraction Result:\n",
      "{'Plan Name': 'Individual Health Insurance', 'Determined Level': 1, 'Input Sections': ['Plan Name'], 'Mapped Target Columns': ['DisCount', 'CoverageLevel'], 'Allowed Target Columns': ['Premium Type', 'DisCount', 'Diseases', 'CoverageLevel', 'PlanFocus', 'Advantage']}\n",
      "\n",
      "Retrieved Record from Datasets:\n",
      "{'Plan Name': 'Individual Health Insurance', 'Premium Type': 'Basic, Lite, Premier', 'DisCount': 34, 'Diseases': \"Acute Myocardial Infarction, Alzheimer's Disease, Angina Pectoris, Aorta Surgery, Aortic Dissection, Atrial Flutter, Brain Surgery, Cancer, Cardiomyopathy, Chronic Liver Disease, Chronic Lung Disease, Congestive Heart Failure, Coronary Artery Disease, Endocarditis, Heart Attack, Kidney Failure, Left Ventricular Hypertrophy, Major Organ Transplant, Motor Neuron Disease, Multiple Sclerosis, Myocarditis, Parkinson's Disease, Pericarditis, Permanent Blindness, Permanent Deafness, Permanent Loss of Speech, Poliomyelitis, Primary Pulmonary Arterial Hypertension, Pulmonary Embolism, Sepsis, Severe Coma, Stroke, Ventricular Fibrillation, Ventricular Tachycardia\", 'CoverageLevel': 'High', 'PlanFocus': 'Individual', 'Advantage': \"Plan 'Individual Health Insurance' (Individual plan) offers premium options (Basic, Lite, Premier) and covers 34 diseases with High coverage. Key diseases include Acute Myocardial Infarction, Alzheimer's Disease, Angina Pectoris, Aorta Surgery, Aortic Dissection, Atrial Flutter, Brain Surgery, Cancer, Cardiomyopathy, Chronic Liver Disease, Chronic Lung Disease, Congestive Heart Failure, Coronary Artery Disease, Endocarditis, Heart Attack, Kidney Failure, Left Ventricular Hypertrophy, Major Organ Transplant, Motor Neuron Disease, Multiple Sclerosis.\"}\n",
      "\n",
      "Final Mapping (Input Tokens -> Output Target Columns):\n",
      "{'Input Tokens': {'Plan Name': 'Individual Health Insurance'}, 'Output Target Columns': {'Premium Type': 'Basic, Lite, Premier', 'DisCount': 34, 'Diseases': \"Acute Myocardial Infarction, Alzheimer's Disease, Angina Pectoris, Aorta Surgery, Aortic Dissection, Atrial Flutter, Brain Surgery, Cancer, Cardiomyopathy, Chronic Liver Disease, Chronic Lung Disease, Congestive Heart Failure, Coronary Artery Disease, Endocarditis, Heart Attack, Kidney Failure, Left Ventricular Hypertrophy, Major Organ Transplant, Motor Neuron Disease, Multiple Sclerosis, Myocarditis, Parkinson's Disease, Pericarditis, Permanent Blindness, Permanent Deafness, Permanent Loss of Speech, Poliomyelitis, Primary Pulmonary Arterial Hypertension, Pulmonary Embolism, Sepsis, Severe Coma, Stroke, Ventricular Fibrillation, Ventricular Tachycardia\", 'CoverageLevel': 'High', 'PlanFocus': 'Individual', 'Advantage': \"Plan 'Individual Health Insurance' (Individual plan) offers premium options (Basic, Lite, Premier) and covers 34 diseases with High coverage. Key diseases include Acute Myocardial Infarction, Alzheimer's Disease, Angina Pectoris, Aorta Surgery, Aortic Dissection, Atrial Flutter, Brain Surgery, Cancer, Cardiomyopathy, Chronic Liver Disease, Chronic Lung Disease, Congestive Heart Failure, Coronary Artery Disease, Endocarditis, Heart Attack, Kidney Failure, Left Ventricular Hypertrophy, Major Organ Transplant, Motor Neuron Disease, Multiple Sclerosis.\"}}\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"what is the total disease covered in the individual health insurance plan?\"\n",
    "print(\"Sample Query:\")\n",
    "print(sample_query)\n",
    "\n",
    "# 1. Run the RAG chain on the combined vector store.\n",
    "rag_result = qa_chain.run(sample_query)\n",
    "print(\"\\nRAG Chain Output:\")\n",
    "print(rag_result)\n",
    "\n",
    "# 2. Run rule-based extraction for the query.\n",
    "extraction_result = generate_final_answer(sample_query)\n",
    "print(\"\\nRule-Based Extraction Result:\")\n",
    "print(extraction_result)\n",
    "\n",
    "# 3. Retrieve a record from the datasets (try each CSV until one matches).\n",
    "def retrieve_record(level, inputs):\n",
    "    for df in [df_level1, df_level2, df_level3, df_level4]:\n",
    "        temp_df = df.copy()\n",
    "        for key, value in inputs.items():\n",
    "            if key in temp_df.columns:\n",
    "                temp_df = temp_df[temp_df[key].str.contains(value, case=False, na=False)]\n",
    "        if not temp_df.empty:\n",
    "            return temp_df.iloc[0].to_dict()\n",
    "    return None\n",
    "\n",
    "record = retrieve_record(extraction_result[\"Determined Level\"], extraction_result)\n",
    "if record is None:\n",
    "    print(\"\\nNo matching record found in the datasets.\")\n",
    "else:\n",
    "    print(\"\\nRetrieved Record from Datasets:\")\n",
    "    print(record)\n",
    "    \n",
    "    # Build final mapping from input tokens to output target columns.\n",
    "    def compute_out_of_pocket(record, amount, premium_type):\n",
    "        premium_type = premium_type.lower()\n",
    "        if \"basic\" in premium_type:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        elif \"lite\" in premium_type:\n",
    "            deductible_pct = 10\n",
    "            copay_pct = 10\n",
    "        elif \"premier\" in premium_type:\n",
    "            deductible_pct = 15\n",
    "            copay_pct = 5\n",
    "        else:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        try:\n",
    "            max_coverage = float(record.get(\"Maximum Coverage\", 0))\n",
    "        except:\n",
    "            max_coverage = 0\n",
    "        if max_coverage > 0 and amount > max_coverage:\n",
    "            return f\"Claim amount {amount} exceeds maximum coverage of {max_coverage}.\"\n",
    "        deductible_value = amount * deductible_pct / 100\n",
    "        updated_amount = amount - deductible_value\n",
    "        copay_value = updated_amount * copay_pct / 100\n",
    "        final_out_of_pocket = deductible_value + copay_value\n",
    "        return final_out_of_pocket\n",
    "\n",
    "    output_mapping = {}\n",
    "    for col in extraction_result[\"Allowed Target Columns\"]:\n",
    "        if col == \"Calculation\":\n",
    "            if \"Amount\" in extraction_result and \"Premium Type\" in extraction_result:\n",
    "                output_mapping[col] = compute_out_of_pocket(record, extraction_result[\"Amount\"], extraction_result[\"Premium Type\"])\n",
    "            else:\n",
    "                output_mapping[col] = None\n",
    "        else:\n",
    "            output_mapping[col] = record.get(col, None)\n",
    "    \n",
    "    final_mapping = {\n",
    "        \"Input Tokens\": {k: extraction_result[k] for k in extraction_result if k in [\"Plan Name\", \"Premium Type\", \"Disease\", \"Amount\"]},\n",
    "        \"Output Target Columns\": output_mapping\n",
    "    }\n",
    "    print(\"\\nFinal Mapping (Input Tokens -> Output Target Columns):\")\n",
    "    print(final_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query:\n",
      "what is the total disease covered in the individual health insurance plan?\n",
      "\n",
      "Extracted Fields:\n",
      "{'Plan Name': 'Individual Health Insurance', 'Determined Level': 1, 'Input Sections': ['Plan Name'], 'Mapped Target Columns': ['DisCount', 'CoverageLevel'], 'Allowed Target Columns': ['Premium Type', 'DisCount', 'Diseases', 'CoverageLevel', 'PlanFocus', 'Advantage']}\n",
      "\n",
      "Augmented Prompt for RAG:\n",
      "Based on the following extracted fields:\n",
      "Plan Name: Individual Health Insurance\n",
      "\n",
      "Answer the query in detail: what is the total disease covered in the individual health insurance plan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lathe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Chain Output:\n",
      "Heart Attack\n"
     ]
    }
   ],
   "source": [
    "# Final Integration: First extract fields, then pass to RAG\n",
    "\n",
    "# Define your sample query\n",
    "sample_query = \"what is the total disease covered in the individual health insurance plan?\"\n",
    "print(\"Sample Query:\")\n",
    "print(sample_query)\n",
    "\n",
    "# Step 1: Run the rule-based extraction to get the fields (columns)\n",
    "extraction_result = generate_final_answer(sample_query)\n",
    "print(\"\\nExtracted Fields:\")\n",
    "print(extraction_result)\n",
    "\n",
    "# Prepare a context string from the extracted fields (only include relevant ones)\n",
    "extracted_context = \"\\n\".join([f\"{field}: {extraction_result[field]}\" \n",
    "                               for field in [\"Plan Name\", \"Premium Type\", \"Disease\", \"Amount\"]\n",
    "                               if field in extraction_result])\n",
    "                               \n",
    "# Step 2: Build an augmented prompt that includes the extracted fields as context\n",
    "augmented_prompt = (\n",
    "    f\"Based on the following extracted fields:\\n{extracted_context}\\n\\n\"\n",
    "    f\"Answer the query in detail: {sample_query}\"\n",
    ")\n",
    "print(\"\\nAugmented Prompt for RAG:\")\n",
    "print(augmented_prompt)\n",
    "\n",
    "# Step 3: Run the RAG chain using the combined vector store QA chain\n",
    "rag_output = qa_chain.run(augmented_prompt)\n",
    "print(\"\\nRAG Chain Output:\")\n",
    "print(rag_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query:\n",
      "what are the advantages of choosing over individual plans?\n",
      "\n",
      "Extracted Fields:\n",
      "{'Determined Level': 1, 'Input Sections': ['Plan Name'], 'Mapped Target Columns': ['Advantage'], 'Allowed Target Columns': ['Premium Type', 'DisCount', 'Diseases', 'CoverageLevel', 'PlanFocus', 'Advantage']}\n",
      "\n",
      "Augmented Prompt for RAG:\n",
      "Based on the following extracted fields:\n",
      "\n",
      "Mapped Target Columns: Advantage\n",
      "\n",
      "Answer the query in detail: what are the advantages of choosing over individual plans?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lathe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Chain Output:\n",
      "Plan 'Individual Health Insurance' (Individual plan)\n",
      "\n",
      "Retrieved Record from Datasets:\n",
      "{'Plan Name': 'Accidental Coverage', 'Premium Type': 'Basic, Lite, Premier', 'DisCount': 20, 'Diseases': 'Accidental Injury, Amputation, Blunt Force Trauma, Burns, Chemical Burns, Compound Fracture, Concussion, Contusions, Crush Injury, Dislocations, Electrical Injury, Fracture, Head Injury, Internal Injury, Lacerations, Open Fracture, Penetrating Wound, Soft Tissue Injury, Spinal Injury, Traumatic Brain Injury', 'CoverageLevel': 'Low', 'PlanFocus': 'Accidental', 'Advantage': \"Plan 'Accidental Coverage' (Accidental plan) offers premium options (Basic, Lite, Premier) and covers 20 diseases with Low coverage. Key diseases include Accidental Injury, Amputation, Blunt Force Trauma, Burns, Chemical Burns, Compound Fracture, Concussion, Contusions, Crush Injury, Dislocations, Electrical Injury, Fracture, Head Injury, Internal Injury, Lacerations, Open Fracture, Penetrating Wound, Soft Tissue Injury, Spinal Injury, Traumatic Brain Injury.\"}\n",
      "\n",
      "Final Mapping (Input Tokens -> Output Target Columns):\n",
      "{'Input Tokens': {}, 'Output Target Columns': {'Premium Type': 'Basic, Lite, Premier', 'DisCount': 20, 'Diseases': 'Accidental Injury, Amputation, Blunt Force Trauma, Burns, Chemical Burns, Compound Fracture, Concussion, Contusions, Crush Injury, Dislocations, Electrical Injury, Fracture, Head Injury, Internal Injury, Lacerations, Open Fracture, Penetrating Wound, Soft Tissue Injury, Spinal Injury, Traumatic Brain Injury', 'CoverageLevel': 'Low', 'PlanFocus': 'Accidental', 'Advantage': \"Plan 'Accidental Coverage' (Accidental plan) offers premium options (Basic, Lite, Premier) and covers 20 diseases with Low coverage. Key diseases include Accidental Injury, Amputation, Blunt Force Trauma, Burns, Chemical Burns, Compound Fracture, Concussion, Contusions, Crush Injury, Dislocations, Electrical Injury, Fracture, Head Injury, Internal Injury, Lacerations, Open Fracture, Penetrating Wound, Soft Tissue Injury, Spinal Injury, Traumatic Brain Injury.\"}}\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"what are the advantages of choosing over individual plans?\"\n",
    "print(\"Sample Query:\")\n",
    "print(sample_query)\n",
    "\n",
    "# 1. Run the rule-based extraction for the query.\n",
    "extraction_result = generate_final_answer(sample_query)\n",
    "print(\"\\nExtracted Fields:\")\n",
    "print(extraction_result)\n",
    "\n",
    "# Build an augmented context string:\n",
    "# Include input fields (from Input Sections) and the mapped target columns.\n",
    "input_context = \"\\n\".join([f\"{field}: {extraction_result[field]}\" \n",
    "                           for field in extraction_result[\"Input Sections\"] \n",
    "                           if field in extraction_result])\n",
    "target_context = \"Mapped Target Columns: \" + \", \".join(extraction_result[\"Mapped Target Columns\"])\n",
    "\n",
    "augmented_prompt = (\n",
    "    f\"Based on the following extracted fields:\\n{input_context}\\n{target_context}\\n\\n\"\n",
    "    f\"Answer the query in detail: {sample_query}\"\n",
    ")\n",
    "print(\"\\nAugmented Prompt for RAG:\")\n",
    "print(augmented_prompt)\n",
    "\n",
    "# 2. Run the RAG chain on the augmented prompt.\n",
    "rag_result = qa_chain.run(augmented_prompt)\n",
    "print(\"\\nRAG Chain Output:\")\n",
    "print(rag_result)\n",
    "\n",
    "# 3. Retrieve a record from the datasets using the rule-based extraction.\n",
    "def retrieve_record(level, inputs):\n",
    "    for df in [df_level1, df_level2, df_level3, df_level4]:\n",
    "        temp_df = df.copy()\n",
    "        for key, value in inputs.items():\n",
    "            if key in temp_df.columns:\n",
    "                temp_df = temp_df[temp_df[key].str.contains(value, case=False, na=False)]\n",
    "        if not temp_df.empty:\n",
    "            return temp_df.iloc[0].to_dict()\n",
    "    return None\n",
    "\n",
    "record = retrieve_record(extraction_result[\"Determined Level\"], extraction_result)\n",
    "if record is None:\n",
    "    print(\"\\nNo matching record found in the datasets.\")\n",
    "else:\n",
    "    print(\"\\nRetrieved Record from Datasets:\")\n",
    "    print(record)\n",
    "    \n",
    "    # 4. Build final mapping from input tokens to output target columns.\n",
    "    def compute_out_of_pocket(record, amount, premium_type):\n",
    "        premium_type = premium_type.lower()\n",
    "        if \"basic\" in premium_type:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        elif \"lite\" in premium_type:\n",
    "            deductible_pct = 10\n",
    "            copay_pct = 10\n",
    "        elif \"premier\" in premium_type:\n",
    "            deductible_pct = 15\n",
    "            copay_pct = 5\n",
    "        else:\n",
    "            deductible_pct = 5\n",
    "            copay_pct = 20\n",
    "        try:\n",
    "            max_coverage = float(record.get(\"Maximum Coverage\", 0))\n",
    "        except:\n",
    "            max_coverage = 0\n",
    "        if max_coverage > 0 and amount > max_coverage:\n",
    "            return f\"Claim amount {amount} exceeds maximum coverage of {max_coverage}.\"\n",
    "        deductible_value = amount * deductible_pct / 100\n",
    "        updated_amount = amount - deductible_value\n",
    "        copay_value = updated_amount * copay_pct / 100\n",
    "        final_out_of_pocket = deductible_value + copay_value\n",
    "        return final_out_of_pocket\n",
    "\n",
    "    output_mapping = {}\n",
    "    for col in extraction_result[\"Allowed Target Columns\"]:\n",
    "        if col == \"Calculation\":\n",
    "            if \"Amount\" in extraction_result and \"Premium Type\" in extraction_result:\n",
    "                output_mapping[col] = compute_out_of_pocket(record, extraction_result[\"Amount\"], extraction_result[\"Premium Type\"])\n",
    "            else:\n",
    "                output_mapping[col] = None\n",
    "        else:\n",
    "            output_mapping[col] = record.get(col, None)\n",
    "    \n",
    "    final_mapping = {\n",
    "        \"Input Tokens\": {k: extraction_result[k] for k in extraction_result if k in [\"Plan Name\", \"Premium Type\", \"Disease\", \"Amount\"]},\n",
    "        \"Output Target Columns\": output_mapping\n",
    "    }\n",
    "    print(\"\\nFinal Mapping (Input Tokens -> Output Target Columns):\")\n",
    "    print(final_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
